{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the IMDB dataset included in Keras.\n",
    "# We want to include the most frequently used 10000 words.\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "# Printing a sample, each list corresponds to a different movie review.\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: 25000\n",
      "X_test: 25000\n"
     ]
    }
   ],
   "source": [
    "# Printing the number of samples.\n",
    "print(f\"X_train: {len(X_train)}\")\n",
    "print(f\"X_test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating X_train and X_test and assing it to a variable X.\n",
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "\n",
    "# Concatenating y_train and y_test and assing it to a variable y.\n",
    "y = np.concatenate((y_train, y_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding all reviews in the X dataset to the length.\n",
    "X = tf.keras.preprocessing.sequence.pad_sequences(X, maxlen=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the training, validation, test datasets.\n",
    "X_train = X[:40000]\n",
    "y_train = y[:40000]\n",
    "\n",
    "X_val = X[40000:45000]\n",
    "y_val = y[40000:45000]\n",
    "\n",
    "X_test = X[45000:50000]\n",
    "y_test= y[45000:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: 40000\n",
      "y_train: 40000\n",
      "X_val: 5000\n",
      "y_val: 5000\n",
      "X_test: 5000\n",
      "y_test: 5000\n"
     ]
    }
   ],
   "source": [
    "# Checking the datasets.\n",
    "print(f\"X_train: {len(X_train)}\")\n",
    "print(f\"y_train: {len(y_train)}\")\n",
    "\n",
    "print(f\"X_val: {len(X_val)}\")\n",
    "print(f\"y_val: {len(y_val)}\")\n",
    "\n",
    "print(f\"X_test: {len(X_test)}\")\n",
    "print(f\"y_test: {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model object.\n",
    "model = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding an embedding layer and a dropout.\n",
    "# Word embedding refers to a numerical representation of words or text.\n",
    "model.add(tf.keras.layers.Embedding(input_dim=10000, output_dim=256))\n",
    "model.add(tf.keras.layers.Dropout(0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a LSTM layer with dropout.\n",
    "model.add(tf.keras.layers.LSTM(256))\n",
    "model.add(tf.keras.layers.Dropout(0.7))\n",
    "\n",
    "# Adding a Dense layer with dropout.\n",
    "model.add(tf.keras.layers.Dense(128, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dropout(0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the output layer.\n",
    "# We using sigmoid activation for binary-classification.\n",
    "model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling and optimizing part.\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model for 5 epochs.\n",
    "results = model.fit(X_train, y_train, epochs=5, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the results.\n",
    "plt.plot(results.history[\"loss\"], label=\"Train\")\n",
    "plt.plot(results.history[\"val_loss\"], label=\"Validation\")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(results.history[\"accuracy\"], label=\"Train\")\n",
    "plt.plot(results.history[\"val_accuracy\"], label=\"Validation\")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the performance.\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making prediction on the reshaped sample.\n",
    "prediction_result = model.predict(X_test[789].reshape(1, 1024))\n",
    "\n",
    "print(f\"Label: {y_test[789]} | Prediction: {prediction_result}\")\n",
    "\n",
    "# The output is smaller than 0.5, so we say it belongs to zero."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
